{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above Threshold Building Energy Usage Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# Importing the dataset\n",
    "\n",
    "df1 = pd.read_csv('../data/year_data/2018-04-01_2018-04-29.csv')\n",
    "df2 = pd.read_csv('../data/year_data/2018-05-01_2018-05-30.csv')\n",
    "df3 = pd.read_csv('../data/year_data/2018-06-01_2018-08-30.csv')\n",
    "df =  pd.concat([df1, df2, df3], ignore_index=True)\n",
    "#df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slices the Dataframe by both Date and Buildings\n",
    "def slice_df(start_date, num_days=7, num_min=0, bldgs=None, df=df):\n",
    "    start, end = slice_df_by_date(start_date, num_days, num_min, df)\n",
    "    return slice_df_by_bldg(bldgs, df.iloc[start:end,:])\n",
    "\n",
    "# Returns starting and ending indexes of data splice\n",
    "def slice_df_by_date(start_date, num_days=7, num_min=0, df=df):\n",
    "    start = df.index[df['time'] == start_date + ' 00:00'].tolist()[0]\n",
    "    end = start + num_days * 96 + num_min % 15\n",
    "    return start, end\n",
    "\n",
    "# Returns df of Selected Columns (None == All)\n",
    "def slice_df_by_bldg(bldgs=None, df=df):\n",
    "    if bldgs == None: return df.iloc[:, 1:]\n",
    "    else: return df[bldgs]\n",
    "\n",
    "# Split Time Stamp Column to Two Columns; Date and Time\n",
    "def split_date_time(df=df):\n",
    "    df['date'] = df['time'].apply(lambda x: x.split(x)[0])\n",
    "    df['time'] = df['time'].apply(lambda x: x.split(x)[1])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Group By Interval with some Method\n",
    "# Methods: Sum, Mean, Min, or Max\n",
    "# Returns a df of grouped data\n",
    "def group_df(method=\"mean\", interval='day', has_time_col=True, df=df):\n",
    "    interval = time_to_row(interval)\n",
    "    grouped_df = pd.DataFrame()   \n",
    "    for i in range(0,len(df)//interval):\n",
    "        if has_time_col: start_date = df['time'][i*interval]\n",
    "        block = df.iloc[ i*interval:(i+1)*interval, : ]\n",
    "        # Perform Computation on Row\n",
    "        if method == \"sum\": block = block.sum(axis=0)\n",
    "        elif method == \"mean\": block = block.mean(axis=0)\n",
    "        elif method == \"min\": block = block.min(axis=0)\n",
    "        elif method == \"max\": block = block.max(axis=0)\n",
    "        else:\n",
    "            print(\"Invalid Method Entry\")\n",
    "            return\n",
    "        # Add the Start Date Label\n",
    "        if has_time_col:\n",
    "            if method == \"mean\": block = pd.Series([start_date]).append(block)  \n",
    "            else: block[0] = start_date\n",
    "        block = block.to_frame().transpose()\n",
    "        grouped_df = grouped_df.append(block)\n",
    "    if method == \"mean\" and has_time_col: grouped_df = grouped_df.rename(columns={ grouped_df.columns[0]: \"time\" })\n",
    "    return grouped_df\n",
    "\n",
    "# Create Training Data\n",
    "def training_data(start_date, y=None, num_days=7, num_min=0, bldgs=None, method=\"mean\", agg_interval='hour',\n",
    "                  time_interval=\"day\", has_time_col=True, df=df):\n",
    "    # Slice Data by Time and Buildings\n",
    "    X = slice_df(start_date, num_days, num_min, bldgs, df)\n",
    "    X = X.rename(columns={ X.columns[0]: \"time\" })\n",
    "    # Aggregate Data on Interval\n",
    "    X = group_df(method=method, interval=agg_interval, has_time_col=False, df=X)\n",
    "    if has_time_col: X = X.drop(columns=['time'])\n",
    "    # Determine Shape of New Dataframe and Reshape\n",
    "    new_col_ct = int(time_to_row(time_interval)/time_to_row(agg_interval))\n",
    "    rows_per_instance = int(X.shape[0]/new_col_ct)\n",
    "    X = X.T.values.reshape(X.shape[1] * rows_per_instance, new_col_ct)\n",
    "    # Return X or both X and updated y if y is given\n",
    "    if y == None: return pd.DataFrame(X)\n",
    "    updated_y = []\n",
    "    for i in y:\n",
    "        for j in range(0, rows_per_instance): updated_y.append(i)\n",
    "    return pd.DataFrame(X), updated_y\n",
    "\n",
    "# Reshape Training Data\n",
    "def reshape_data(df, has_time_col=True, agg_interval=\"0:15\", time_interval=\"day\"):\n",
    "    X = df\n",
    "    if has_time_col: X = X.drop(columns=['time'])\n",
    "    # Determine Shape of New Dataframe and Reshape\n",
    "    new_col_ct = int(time_to_row(time_interval)/time_to_row(agg_interval))\n",
    "    rows_per_instance = int(X.shape[0]/new_col_ct)\n",
    "    X = X.T.values.reshape(X.shape[1] * rows_per_instance, new_col_ct)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a time interval into the correct number of rows\n",
    "# Interval: \"3:15\", \"hour\", day\", \"week\", \"month\", \"year\" \n",
    "def time_to_row(interval):\n",
    "    time_conv =\t{ \"year\": 35040, \"month\": 2880, \"week\": 672, \"day\": 96, \"hour\": 4}\n",
    "    if interval in time_conv: return time_conv[interval]\n",
    "    elif \":\" in interval: return int(interval.split(':')[0])*4 + int(interval.split(':')[1])//15\n",
    "    else: return\n",
    "\n",
    "# Converts the Date into a Number\n",
    "def date_to_num(date):\n",
    "    d,t = date.split()\n",
    "    y,m,day = d.split('-')\n",
    "    hr, minute = t.split(':')\n",
    "    return int(y + m + day + hr + minute)\n",
    "\n",
    "# Converts Traditional Date Format '4/4/2018' to ISO '2018-04-01'\n",
    "def date_to_iso(date):\n",
    "    month, day, year = date.split('/')\n",
    "    if (int(month) < 10): month = '0' + str(int(month))\n",
    "    if (int(day) < 10): day = '0' + str(int(day))\n",
    "    return year + '-' + month + '-' + day\n",
    "\n",
    "# Converts a Date Number to Proper Date Format\n",
    "def num_to_date(date):\n",
    "    date = str(date)\n",
    "    return date[0:4] + '-' + date[4:6] + '-' + date[6:8] + ' ' + date[8:10] + ':' + date[10:12]\n",
    "\n",
    "# Convert Number to Building Type Classification\n",
    "def num_to_label(num):\n",
    "    conv = {0: 'Cocentric', 1: 'People', 2: 'Schedule', 3: 'Reverse', 4: 'Random'}\n",
    "    if num in conv: return conv[num]\n",
    "    return \"Invalid Entry\"\n",
    "\n",
    "# Returns rotates array n times, clockwise by default\n",
    "def rotate_list(arr, n):\n",
    "    arr = list(arr)\n",
    "    if n >= 0: return arr[-n:] + arr[:-n]\n",
    "    return arr[abs(n):] + arr[:abs(n)]\n",
    "\n",
    "# Convert From LabelEncoded to OneHotEncoded\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "def label_to_one_hot(y):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    return one_hot_encoder.fit_transform(y).toarray()\n",
    "\n",
    "# Convert From OneHotEncoded to LabelEncoded\n",
    "def one_hot_to_label(y):\n",
    "    return [ np.argmax(i) for i in y ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fingerprint Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint_draw(bldg, save=False, show=False, df=df):\n",
    "    r = df[bldg]\n",
    "    theta = []\n",
    "    for i in range(0, len(r)):\n",
    "        theta.append((i%96)/96 * 2 * np.pi)\n",
    "    r = r.append(pd.Series(r.values[0]))\n",
    "    theta.append(0)\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111, projection='polar', facecolor='white')\n",
    "    ax.plot(theta, r, c='y')\n",
    "    ax.set_xticklabels(['12am', '3am', '6am', '9am', '12pm', '3pm', '6pm', '9pm'])\n",
    "    plt.figure(figsize=(4,4))\n",
    "    ax.set_rlabel_position(90)\n",
    "    ax.grid(True)\n",
    "    ax.grid(linewidth=.3)\n",
    "    ax.set_title(\"Energy Usage of Building \" + bldg + \" (kWh)\", va='bottom')\n",
    "    ax.title.set_position([.5, 1.15])\n",
    "    if save: fig.savefig('fingerprints/' + bldg + '.png', bbox_inches='tight')\n",
    "    if show: plt.show()\n",
    "\n",
    "#fingerprint('2018-02-08', 1, bldgs=['B104'], save=False)\n",
    "#fingerprint('2018-01-01', 1)\n",
    "#fingerprint('2018-01-01', num_days=5, save=True)\n",
    "def fingerprint(start_date, num_days=7, num_min=0, bldgs=None, save=True, df=df):\n",
    "    df = slice_df(start_date, num_days, num_min, bldgs, df)    \n",
    "    for bldg in df.columns: fingerprint_draw(bldg, save, df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a Stacked dataframe with each rotation of each row\n",
    "def rotate_data(X, y, iterations = 1, ret_orig = True):\n",
    "    final_X, final_y = pd.DataFrame(), pd.DataFrame()\n",
    "    for itr in range(0, iterations):\n",
    "        new_df = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            new_df.append(rotate_list( X.iloc[i, :].values, itr - (iterations // 2) ))\n",
    "        final_X = pd.concat([final_X, pd.DataFrame(new_df)])\n",
    "        final_y = pd.concat([final_y, pd.DataFrame(y)])\n",
    "    if ret_orig: return pd.concat([X, final_X]), pd.concat([y, final_y])\n",
    "    return final_X, final_y\n",
    "\n",
    "# Remove Buildings With Energy Usage Below 60 kwH\n",
    "def remove_small(X, y, thresh=75):\n",
    "    final_X, final_y = pd.DataFrame(), pd.DataFrame()\n",
    "    y_list = list(y.iloc[:, 0])\n",
    "    new_df = []\n",
    "    new_y = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        if X.iloc[i, :].mean() > thresh:\n",
    "            new_df.append(X.iloc[i, :].values)\n",
    "            new_y.append(y_list[i])\n",
    "    final_X = pd.concat([final_X, pd.DataFrame(new_df)])\n",
    "    final_y = pd.concat([final_y, pd.DataFrame(new_y)])\n",
    "    return final_X, final_y\n",
    "\n",
    "\n",
    "# Subtract Row Elements by Min Value in Row\n",
    "def minus_min(X):\n",
    "    new_X = []\n",
    "    for i in range(0, X.shape[0]): new_X.append(X.iloc[i, :] - min(X.iloc[i, :]))\n",
    "    return pd.DataFrame(new_X)\n",
    "\n",
    "# Normalize Each Row Independently\n",
    "from sklearn.preprocessing import normalize\n",
    "def norm_rows(X):\n",
    "    new_X = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        new_row = [list(X.iloc[i, :])]\n",
    "        new_row = normalize(new_row)\n",
    "        new_X.append(new_row)\n",
    "    return pd.DataFrame(new_X)\n",
    "\n",
    "# Square Each Row Independently\n",
    "def square_rows(X):\n",
    "    new_X = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        new_row = X.iloc[i, :]\n",
    "        for i,v in enumerate(new_row):\n",
    "            new_row[i] = v*v\n",
    "        new_X.append(new_row)\n",
    "    return pd.DataFrame(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use this function for non-binary classification\n",
    "def confusion_matrix_multi(y_test, y_pred):\n",
    "    if pd.DataFrame(y_pred).shape[1] == 1:\n",
    "        y_pred = label_to_one_hot(pd.DataFrame(y_pred))\n",
    "    conf_mat = confusion_matrix(one_hot_to_label(y_test), one_hot_to_label(y_pred))\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    classes = ['Cocentric', 'People', 'Schedule', 'Reverse', 'Random']\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "                    xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    accuracy = conf_mat_acc(conf_mat)\n",
    "    plt.title('Accuracy: ' + str(accuracy) + '%', fontsize=20)\n",
    "    plt.show()\n",
    "    return accuracy\n",
    "\n",
    "# Calculates the Accuracy for any n by n conf matrix\n",
    "def conf_mat_acc(conf_mat):\n",
    "    acc = 0\n",
    "    for i in range(0, len(conf_mat)):\n",
    "        for j in range(0, len(conf_mat)):\n",
    "            if i == j: acc += conf_mat[i][j]\n",
    "    acc = acc/sum(sum(conf_mat))\n",
    "    acc = round(acc * 100, 2)\n",
    "    return acc\n",
    "\n",
    "# Print All Misclassified Instances\n",
    "def print_misclassified(predicted, actual, X_test, y_test, y_pred, sc, relabel=False):\n",
    "    y_test = one_hot_to_label(y_test)\n",
    "    y_pred = one_hot_to_label(y_pred)\n",
    "    X_test = pd.DataFrame(sc.inverse_transform(X_test))\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        if(y_pred[i] == predicted and y_test[i] == actual):\n",
    "            title = \"Predicted: \" + num_to_label(predicted) + \". Actual: \" + num_to_label(actual)\n",
    "            fingerprint_draw_r( title, X_test.iloc[i, :], save=False, show=True, df=df)\n",
    "            if relabel: search_fingerprint(X_test.iloc[i, :])\n",
    "                \n",
    "# Search Data For Fingerprint Match to Relabel the Classification\n",
    "def search_fingerprint(data, csv='labeled_data.csv'):\n",
    "    labeled_df = pd.read_csv(csv)\n",
    "    labeled_dates = labeled_df[labeled_df.columns[0]]\n",
    "    relabel_val = input(\"Enter Relabel Value (Enter 'n' to skip): \")\n",
    "    no_match = True\n",
    "    if relabel_val in ['0', '1', '2', '3', '4']:\n",
    "        for date in labeled_dates:\n",
    "            sliced_df = slice_df(date_to_iso(date), num_days=1, df=df)\n",
    "            sliced_df = sliced_df.reset_index(drop=True)\n",
    "            for col in sliced_df:\n",
    "                if round(data[0], 2) == sliced_df.at[0, col]:\n",
    "                    for i in range(0,96):\n",
    "                        if( round(data[i], 2) != sliced_df.at[i, col]): break\n",
    "                        if(i == 95): \n",
    "                            no_match = False\n",
    "                            print(\"Building: \" + col + ' on ' + date + ' updated to ' + str(relabel_val))\n",
    "                            update_label(col, date, int(relabel_val))\n",
    "    if no_match: print('No Matches')\n",
    "\n",
    "# Print all Misclassified Instances\n",
    "def print_all_misclassified(X_test, y_test, y_pred, sc):\n",
    "    for i in range(0, 5):\n",
    "        for j in range(0, 4):\n",
    "            if i != j: print_misclassified(i, j, X_test, y_test, y_pred, sc, relabel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bldg - str, date (mm/dd/yy) - str, update_val (label 0-4) - str, csv (file to update) - string\n",
    "def update_label(bldg, date, update_val, csv='labeled_data.csv'):\n",
    "    df = pd.read_csv(csv)\n",
    "    index = df.index[df[df.columns[0]] == date].tolist()[0]\n",
    "    df.at[index, bldg] = update_val\n",
    "    df.to_csv(csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Feature Importances After Creating a Decision Tree Classifying Object\n",
    "def plot_feature_importances(model, n_features):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), df.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.title(\"Decision Tree Feature Importance\")\n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Test Different Values of k for K-NN Classification\n",
    "def knn_test_k(start=1, end=10):\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    neighbors_settings = range(start, end)\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        knn = KNeighborsClassifier(p=2, n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, y_train)\n",
    "        training_accuracy.append(knn.score(X_train, y_train))\n",
    "        test_accuracy.append(knn.score(X_test, y_test))\n",
    "    plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "    plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return test_accuracy.index(max(test_accuracy)) + 1\n",
    "    #plt.savefig('knn_compare_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#update_label('B110', '4/6/2018', 0)\n",
    "y_labels = pd.read_csv('../data/labeled_data.csv')\n",
    "#y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = y_labels.iloc[:3, :]\n",
    "y1 = y1.melt()\n",
    "y1 = pd.DataFrame(y1.iloc[3:len(y1)-3,1])\n",
    "\n",
    "# Get Original Data\n",
    "X1 = training_data(start_date='2018-04-04', y=None, num_days=3, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y1.shape[0] == X1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y_labels.iloc[3:8, :]\n",
    "y2 = y2.melt()\n",
    "y2 = pd.DataFrame(y2.iloc[5:len(y2)-5,1])\n",
    "\n",
    "X2 = training_data(start_date='2018-04-09', y=None, num_days=5, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y2.shape[0] == X2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y_labels.iloc[8:10, :]\n",
    "y3 = y3.melt()\n",
    "y3 = pd.DataFrame(y3.iloc[2:len(y3)-2, 1])\n",
    "\n",
    "X3 = training_data(start_date='2018-06-21', y=None, num_days=2, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y3.shape[0] == X3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4 = y_labels.iloc[10:15, :]\n",
    "y4 = y4.melt()\n",
    "y4 = pd.DataFrame(y4.iloc[5:len(y4)-5, 1])\n",
    "\n",
    "X4 = training_data(start_date='2018-06-25', y=None, num_days=5, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "        time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y4.shape[0] == X4.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y5 = y_labels.iloc[15:17, :]\n",
    "y5 = pd.DataFrame(y5).melt()\n",
    "y5 = pd.DataFrame(y5.iloc[2:len(y5)-2, 1])\n",
    "\n",
    "X5 = training_data(start_date='2018-07-02', y=None, num_days=2, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y5.shape[0] == X5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y6 = y_labels.iloc[17:19, :]\n",
    "y6 = pd.DataFrame(y6).melt()\n",
    "y6 = pd.DataFrame(y6.iloc[2:len(y6)-2, 1])\n",
    "\n",
    "X6 = training_data(start_date='2018-07-05', y=None, num_days=2, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y6.shape[0] == X6.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y7 = y_labels.iloc[19:22, :]\n",
    "y7 = pd.DataFrame(y7).melt()\n",
    "y7 = pd.DataFrame(y7.iloc[3:len(y7)-3, 1])\n",
    "\n",
    "X7 = training_data(start_date='2018-07-09', y=None, num_days=3, num_min=0, method=\"mean\", agg_interval='0:15',\n",
    "                  time_interval=\"day\", has_time_col=False, df=df)\n",
    "\n",
    "y7.shape[0] == X7.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2926, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.concat([y1, y2, y3, y4, y5, y6, y7], axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2926, 96)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X1, X2, X3, X4, X5, X6, X7], axis=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Buildings With Energy Usage Below Threshold\n",
    "X, y = remove_small(X, y, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1076, 96)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encode Y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = label_to_one_hot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0) # Original 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shapes:\n",
      "(11836, 96)\n",
      "(11836, 5)\n"
     ]
    }
   ],
   "source": [
    "# Multiply Datasize\n",
    "X_train_n, y_train_n = rotate_data(pd.DataFrame(X_train), pd.DataFrame(y_train), 10, ret_orig=False)\n",
    "X_train = pd.concat([pd.DataFrame(X_train), X_train_n])\n",
    "y_train = pd.concat([pd.DataFrame(y_train), y_train_n])\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "print('Final Shapes:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Fitting Decision Tree Classification to the Training set\\nfrom sklearn.tree import DecisionTreeClassifier\\nclassifier = DecisionTreeClassifier(criterion = 'entropy', min_impurity_split=0.27)\\nclassifier.fit(X_train, y_train)\\ny_pred = classifier.predict(X_test)\\nconfusion_matrix_multi(y_test, y_pred)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', min_impurity_split=0.27)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "confusion_matrix_multi(y_test, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print_all_misclassified(X_test, y_test, y_pred, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_feature_importances(classifier,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.externals.six import StringIO  \\nfrom IPython.display import Image  \\nfrom sklearn.tree import export_graphviz\\nimport pydotplus\\ndot_data = StringIO()\\nexport_graphviz(classifier, out_file=dot_data,  \\n                filled=True, rounded=True,\\n                special_characters=True)\\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \\nImage(graph.create_png())\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize Decision Tree\n",
    "'''\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(classifier, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.ensemble import RandomForestClassifier\\nrf_classifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy') \\nrf_classifier.fit(X_train, y_train)\\ny_pred = rf_classifier.predict(X_test)\\nconfusion_matrix_multi(y_test, y_pred)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy') \n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "confusion_matrix_multi(y_test, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_all_misclassified(X_test, y_test, y_pred, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_k = knn_test_k()'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''best_k = knn_test_k()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Fitting K-NN to the Training set\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn_classifier = KNeighborsClassifier(n_neighbors = best_k, metric = 'minkowski', p = 2)\\nknn_classifier.fit(X_train, y_train)\\ny_pred = knn_classifier.predict(X_test)\\nprint('Optimal k Value: ' + str(best_k))\\nconfusion_matrix_multi(y_test, y_pred)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = best_k, metric = 'minkowski', p = 2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "print('Optimal k Value: ' + str(best_k))\n",
    "confusion_matrix_multi(y_test, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_all_misclassified(X_test, y_test, y_pred, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neural_network import MLPClassifier\\nmlp = MLPClassifier(hidden_layer_sizes=(30, 20, 10, 8), max_iter=300)\\nmlp.fit(X_train,y_train)\\ny_pred = mlp.predict(X_test)\\nconfusion_matrix_multi(y_test, y_pred)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30, 20, 10, 8), max_iter=300)\n",
    "mlp.fit(X_train,y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "confusion_matrix_multi(y_test, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Voting Ensemble for Classification\\nfrom sklearn import model_selection\\nfrom sklearn.ensemble import VotingClassifier\\nseed = 7\\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\\n# create the sub models\\nestimators = []\\nestimators.append(('Random Forest', rf_classifier))\\nestimators.append(('KNN', knn_classifier))\\nestimators.append(('ANN', mlp))\\n# create the ensemble model\\nensemble = VotingClassifier(estimators)\\n#results = model_selection.cross_val_score(ensemble, X_train, one_hot_to_label(y_train), cv=kfold)\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Voting Ensemble for Classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "estimators.append(('Random Forest', rf_classifier))\n",
    "estimators.append(('KNN', knn_classifier))\n",
    "estimators.append(('ANN', mlp))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "#results = model_selection.cross_val_score(ensemble, X_train, one_hot_to_label(y_train), cv=kfold)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nensemble = ensemble.fit(X_train, one_hot_to_label(y_train))\\ny_pred_ensemble = ensemble.predict(X_test)\\nconfusion_matrix_multi(y_test, y_pred_ensemble)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ensemble = ensemble.fit(X_train, one_hot_to_label(y_train))\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "confusion_matrix_multi(y_test, y_pred_ensemble)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Save Results\\nimport pickle\\n# save the model to disk\\nfilename = 'building_classification_model.p'\\npickle.dump(ensemble, open(filename, 'wb'))\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Save Results\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'building_classification_model.p'\n",
    "pickle.dump(ensemble, open(filename, 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset\n",
    "df_8mo = pd.read_csv('../data/year_data/2018-01-01_2018-08-30.csv')\n",
    "\n",
    "# Replace Nan's with 0\n",
    "for i in range(1, df_8mo.shape[1]): df_8mo.iloc[1:, i] = df_8mo.iloc[1:, i].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Data for Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Format Energy Consumption Data\n",
    "energy_data = reshape_data(df_8mo)\n",
    "original_len = len(energy_data)\n",
    "\n",
    "energy_y = []\n",
    "for i in range(len(energy_data)): energy_y.append(i)\n",
    "energy_y = pd.DataFrame(energy_y)\n",
    "\n",
    "energy_data, energy_y = remove_small(pd.DataFrame(energy_data), energy_y, thresh=125)\n",
    "energy_y = energy_y.iloc[:, 0]\n",
    "\n",
    "sc = StandardScaler()\n",
    "energy_data = sc.fit_transform(energy_data)\n",
    "\n",
    "# Get List of Buildings\n",
    "buildings = list(df_8mo)[1:]\n",
    "\n",
    "# Get List of dates\n",
    "dates = pd.DataFrame(df_8mo.iloc[:,0])\n",
    "dates = group_df(method=\"mean\", interval='day', has_time_col=True, df=dates)\n",
    "for i in range(dates.shape[0]): dates.iloc[i, 0] = dates.iloc[i, 0].split()[0]\n",
    "dates = list(dates.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kunal\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "filename = 'building_classification_model.p'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "classifications = loaded_model.predict(energy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = ['1/1/2016', '1/2/2016', '1/18/2016', '3/21/2016', '3/22/2016', '3/23/2016',\n",
    "            '3/24/2016', '3/25/2016','9/5/2016', '10/10/2016', '10/11/2016', '11/23/2016',\n",
    "            '11/24/2016', '11/25/2016','12/25/2016', '12/26/2016', '12/27/2016',\n",
    "            '12/28/2016', '12/29/2016', '12/30/2016','12/31/2016', '1/1/2017', '1/2/2017',\n",
    "            '1/16/2017', '3/19/2017', '3/20/2017','3/21/2017', '3/22/2017', '3/23/2017',\n",
    "            '3/24/2017', '5/28/2017', '7/3/2017', '7/4/2017', '9/4/2017', '10/9/2017',\n",
    "            '10/10/2017', '11/22/2017', '11/23/2017', '11/24/2017', '12/25/2017', '12/27/2017',\n",
    "            '12/28/2017', '12/29/2017', '12/30/2017','12/31/2017', '1/1/2018', '1/2/2018',\n",
    "           '1/15/2018', '3/19/2018', '3/20/2018', '3/21/2018', '3/22/2018', '3/23/2018',\n",
    "           '5/28/2018', '7/3/2018', '7/4/2018', '9/3/2018', '10/8/2018', '10/9/2018',\n",
    "           '11/21/2018', '11/22/2018', '11/23/2018', '12/24/2018', '12/25/2018',\n",
    "            '12/26/2018', '12/27/2018', '12/28/2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Day Types. 1-5: M-F, -1: Weekend, -2: Holiday\n",
    "# Param dates: List of Dates in 'MM/DD/YYYY' Format\n",
    "# Param holidays: List of Holidays in 'MM/DD/YY' Format\n",
    "from datetime import date\n",
    "def classify_day_type(dates, holidays):\n",
    "    day_type = []\n",
    "    for i in dates:\n",
    "        date_elem = i.split('/')\n",
    "        day_of_week = date(int(date_elem[2]), int(date_elem[0]), int(date_elem[1])).isoweekday()\n",
    "        if day_of_week > 5: day_type.append(-1)\n",
    "        elif i in holidays: day_type.append(-2)\n",
    "        else: day_type.append(day_of_week)\n",
    "    return day_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create building_type Array\n",
    "building_type = []\n",
    "for i in range(original_len): building_type.append(-1)\n",
    "for i in range(len(energy_y)): building_type[energy_y[i]] = classifications[i]\n",
    "    \n",
    "# Create Classification df Date, Building Type\n",
    "dates = dates * 134\n",
    "updated_bldg = [] \n",
    "for i in buildings: updated_bldg += [i] * 237\n",
    "    \n",
    "# Create List of Day Types\n",
    "day_type = classify_day_type(dates, holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Building Classification Dataframe\n",
    "day_type_df = pd.DataFrame(day_type, columns=['Day_Type'])\n",
    "dates_df = pd.DataFrame(dates, columns=['Date'])\n",
    "updated_bldg_df = pd.DataFrame(updated_bldg, columns=['Building'])\n",
    "building_type_df = pd.DataFrame(building_type, columns=['Type'])\n",
    "bldg_classes = pd.concat([day_type_df, dates_df, updated_bldg_df, building_type_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "bldg_classes.to_csv('building_classifications_1-1-2018_8-30-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from CSV\n",
    "bldg_casses = pd.read_csv('building_classifications_1-1-2018_8-30-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Weekends and Holidays from the Dataset\n",
    "wkday_bldg = bldg_classes[bldg_classes['Day_Type'] > 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Classification Distribution Analysis of a List\n",
    "# param arr (array): List of elements to be analyzed\n",
    "import collections\n",
    "def distr_analysis(arr):\n",
    "    print('Data Distribution Analysis' + '\\n')\n",
    "    final_counts = dict(collections.Counter(arr))\n",
    "    print(final_counts)\n",
    "    total = len(arr)\n",
    "    for k,v in final_counts.items(): print('Type ' + str(k) + ': ' + str(round(v/total * 100, 2)) + '%' )\n",
    "    plt.hist(arr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution Analysis\n",
      "\n",
      "{-1: 20361, 0: 9368, 1: 1400, 4: 100, 2: 522, 3: 7}\n",
      "Type -1: 64.11%\n",
      "Type 0: 29.5%\n",
      "Type 1: 4.41%\n",
      "Type 4: 0.31%\n",
      "Type 2: 1.64%\n",
      "Type 3: 0.02%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore Distribution of Building Classification\n",
    "distr_analysis(bldg_classes['Type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def: Graph Building Type over List of Dates\n",
    "# Param dates (arr): List of dates\n",
    "# Param types (arr): List of building classification types\n",
    "# Param bldg_name (str): Name of the Building\n",
    "# Param show (bool): Plots Graph if True\n",
    "# Param save (bool): Save Image of Plot \n",
    "def graph_bldg_type(dates, types, bldg_name, show=False, save=False):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(dates, types)\n",
    "    plt.title(bldg_name)\n",
    "    plt.xlabel('Date')\n",
    "    plt.yticks([-1, 0, 1, 2, 3, 4])\n",
    "    plt.ylabel('Classification')\n",
    "    if show: plt.show()\n",
    "    if save: fig.savefig( bldg_name + '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom matplotlib import dates as mpl_dates\\ndef convert_to_matplot_date(date_x):\\n    date_elem = date_x.split('/')\\n    python_date = date(int(date_elem[2]), int(date_elem[0]), int(date_elem[1]))\\n    matplot_date = mpl_dates.date2num(python_date)\\n    print(matplot_date)\\n    return matplot_date\\n\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Date String into Matplot Date Object\n",
    "# Param date (str): In form \"MM/DD/YYYY\"\n",
    "from matplotlib import dates as mpl_dates\n",
    "def convert_to_matplot_date(date_x):\n",
    "    date_elem = date_x.split('/')\n",
    "    python_date = date(int(date_elem[2]), int(date_elem[0]), int(date_elem[1]))\n",
    "    matplot_date = mpl_dates.date2num(python_date)\n",
    "    return matplot_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Building Types over Time Given a Dataframe with the Columns: 'Building', 'Data', 'Type'\n",
    "# param bldg_df (dataframe): dataframe with the building data\n",
    "# param show (bool): Displays Plot\n",
    "# param save (bool): Saves plot to the current directory\n",
    "def plot_building_type(bldg_df, show=True, save=False):\n",
    "    prev_bldg = bldg_df['Building'][0]\n",
    "    dates = []\n",
    "    bldg_type = []\n",
    "    for i in range(len(bldg_df)):    \n",
    "        curr_bldg = bldg_df['Building'][i]\n",
    "        # Graph Old Building if Current Bldg is New or Last Bldg\n",
    "        if curr_bldg != prev_bldg or i == len(bldg_df):\n",
    "            graph_bldg_type(dates, bldg_type, prev_bldg, show=show, save=save)\n",
    "            dates = []\n",
    "            bldg_type = []\n",
    "            prev_bldg = curr_bldg\n",
    "        else:\n",
    "            dates.append(bldg_df['Date'][i])\n",
    "            bldg_type.append(bldg_df['Type'][i])\n",
    "\n",
    "#plot_building_type(wkday_bldg, show=False, save=True)\n",
    "#print('Saving Complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
